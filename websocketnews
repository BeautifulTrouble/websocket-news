#!/usr/bin/env python
#encoding: utf-8

import autovenv
autovenv.run()

import atexit
import datetime
import json
import multiprocessing
import re
import ssl
import time
import uuid
from collections import deque, namedtuple
from functools import partial
from urllib.parse import urlsplit

import bleach
import dateutil.parser as dateparser
import facebook
import requests
import tweepy
import websocket
from bleach.sanitizer import Cleaner
from bleach.linkifier import LinkifyFilter
from plucky import plucks
from tweepy.streaming import Stream, StreamListener
from SimpleWebSocketServer import SimpleWebSocketServer, SimpleSSLWebSocketServer, WebSocket

from utils import (
    DEBUG,
    raise_hell,
    script_directory,
)

from config import (
    CONFIG_JSON_URL,
    FACEBOOK_API_VERSION,
    FACEBOOK_APP_ID,
    FACEBOOK_SECRET,
    NEWSFEED_ITEM_LIMIT,
    RECONFIGURE_SECONDS,
    SSL_ENABLED,
    SSL_KEYFILE,
    SSL_CERTFILE,
    TWITTER_CONSUMER_KEY,
    TWITTER_CONSUMER_SECRET,
    TWITTER_ACCESS_TOKEN,
    TWITTER_ACCESS_TOKEN_SECRET,
    WEBSOCKET_PORT,
)

CACHE_FILE = 'cache.json'

# Configuration parsed 
fallback_config = requests.get(CONFIG_JSON_URL).json()

# Push all news items through this ipc Queue
newsQ = multiprocessing.Queue()

# This merely serves to document the properties of news items
NewsItem = namedtuple('NewsItem', [
    'type',
    'content', 
    'content_html',
    'follow_link',
    'image_http', 
    'image_https',
    'timestamp',
    'user_name', 
    'user_handle', 
    'user_link',
])


# So we can offer a safe HTML representation of news items
# TODO: Create custom filter to linkify #hashtags & @usernames
clean_html = Cleaner(filters=[LinkifyFilter]).clean

# For sorting lists of NewsItems
timestamp_sorted = lambda c: sorted(c, key=lambda i: i['timestamp'])

_ws_commands = {c: str(uuid.uuid4()) for c in [
    'new',
]}

def ws_command(cmd_name_or_instance, *, 
        token_getter=lambda inst: inst.request.headers['cookie']):
    '''
    This is an IPC mechanism which allows processes to signal a blocking
    websocket loop by exchanging random pregenerated uuid tokens representing
    commands which can then be managed by a handshake callback.

        to send a command:
            ws_command('reload_config')

        to receive an incoming command:
            ws_command(self, token_getter=lambda s: s.token)

    Keep in mind that the _ws_commands dictionary becomes effectively immutable
    after multiprocessing calls fork(). The uuid tokens must be pregenerated 
    in order to be shared between a sender and receiver
    '''

    # A command is being sent
    if isinstance(cmd_name_or_instance, str):
        cmd_name = cmd_name_or_instance
        if cmd_name not in _ws_commands:
            raise Exception('Unknown ws_command "{}"'.format(cmd_name))

        if SSL_ENABLED:
            url = 'wss://localhost:{}'.format(WEBSOCKET_PORT)
            ws = websocket.create_connection(url, cookie=_ws_commands[cmd_name],
                sslopt={"check_hostname": False})
        else:
            url = 'ws://localhost:{}'.format(WEBSOCKET_PORT)
            ws = websocket.create_connection(url, cookie=_ws_commands[cmd_name])
        time.sleep(.3)
        ws.close()

    # A command is being received
    elif not isinstance(cmd_name_or_instance, str):
        instance = cmd_name_or_instance
        # The token_getter function defaults to assuming an instance of 
        # SimpleWebSocketServer.WebSocket with token stored in a request 
        # header
        cmd_token = token_getter(instance)
        return ([k for k,v in _ws_commands.items() if cmd_token == v] or [''])[0]




class NewsfeedServer(WebSocket):
    # Connected clients 
    client_states = {}

    # Old items stored in this queue are discarded when queue is full
    twitter_news = deque(maxlen=NEWSFEED_ITEM_LIMIT)
    facebook_news = deque(maxlen=NEWSFEED_ITEM_LIMIT)

    def __init__(self, *a, **kw):
        super().__init__(*a, **kw)

    @classmethod
    def save_cache(cls):
        with script_directory():
            all_news = timestamp_sorted(cls.twitter_news + cls.facebook_news)
            try:
                with open(CACHE_FILE, 'w') as cache:
                    cache.write(json.dumps(all_news))
            except OSError: return
        print('Saved {} items to cache'.format(len(all_news)))

    @classmethod
    def load_cache(cls):
        with script_directory():
            try:
                with open(CACHE_FILE) as cache:
                    all_news = json.loads(cache.read())
            except OSError: return
            cls.twitter_news = timestamp_sorted(filter(lambda i: i['type'] == 'twitter', all_news))
            cls.facebook_news = timestamp_sorted(filter(lambda i: i['type'] == 'facebook', all_news))
        print('Loaded {} items from cache'.format(len(all_news)))

    @raise_hell
    def handleConnected(self):
        '''
        Update the list of clients and send NEWSFEED_ITEM_LIMIT many messages
        '''
        # When other processes need to signal this server, they can create a
        # random uuid and attach it to the cookie header of a websocket connection
        if ws_command(self) == 'new':
            news_item = newsQ.get()
            if news_item['type'] == 'twitter':
                self.twitter_news.append(news_item)
            elif news_item['type'] == 'facebook':
                self.facebook_news.append(news_item)
            else: # ???
                return

            recipient_count = 0
            for client, started in self.client_states.items():
                if started:
                    text = json.dumps([news_item])
                    client.sendMessage(text)
                    recipient_count += 1
            print('Sent "{}..." to {} recipients'.format(news_item['content'].replace('\n', '')[:50], recipient_count))

        # A new client has connected so we add its instance to the client state
        # dict and set its state to False meaning "don't send it messages yet"
        else:
            self.client_states[self] = False

    @raise_hell
    def handleMessage(self):
        '''
        A client sends a message to begin receiving news posts
        '''
        # If the client has sent anything - "start", "ready", "yo!", we'll send
        # a list of messages from the class attribute news, a queue containing
        # NEWSFEED_ITEM_LIMIT many messages.
        all_news = timestamp_sorted(self.twitter_news + self.facebook_news)
        text = json.dumps([*all_news])
        self.sendMessage(text)
        print('Sent {} items to new recipient'.format(len(all_news)))

        # Signify that this client is now in the started state
        self.client_states[self] = True

    @raise_hell
    def handleClose(self):
        self.client_states.pop(self, None)




class TwitterStreamListener(StreamListener):
    '''
    Collect tweets and alert the websocket server process (via a websocket)
    '''

    def __init__(self, *a, **kw):
        self.start_time = time.time()
        super().__init__(*a, **kw)

    def on_error(self, status_code):
        '''
        Slow down and disconnect on rate limit. The expectation here is that
        the stream will be restarted immediately.
        '''
        if status_code == 420:
            time.sleep(5)
            return False

    def on_status(self, status):
        '''
        Produce a json-string of the message and notify the websocket server
        to broadcast it to clients
        '''
        tweet = NewsItem(
            type='twitter',
            content=status.text,
            content_html=clean_html(status.text),
            follow_link='https://twitter.com/intent/follow?screen_name={}'.format(status.author.screen_name),
            image_http=status.author.profile_image_url,
            image_https=status.author.profile_image_url_https,
            timestamp=status.timestamp_ms,
            user_name=status.author.name,
            user_handle=status.author.screen_name,
            user_link='https://twitter.com/{}'.format(status.author.screen_name),
        )._asdict()
        newsQ.put(tweet)

        # Signal to the NewsfeedServer process that new data is in the queue
        ws_command('new')

        # Disconnect every RECONFIGURE_SECONDS so that the blocking call to
        # stream.userstream() will return and we can update config data
        if time.time() - self.start_time >= RECONFIGURE_SECONDS:
            return False



def the_twitter_cycle():
    '''
    Instantiate a twitter stream listener which should terminate itself as
    often as it wants to check for new configuration data
    '''

    auth = tweepy.OAuthHandler(TWITTER_CONSUMER_KEY, TWITTER_CONSUMER_SECRET)
    auth.set_access_token(TWITTER_ACCESS_TOKEN, TWITTER_ACCESS_TOKEN_SECRET)
    api = tweepy.API(auth)

    first_run = True

    # Repeatedly re-configure and initiate stream about every RECONFIGURE_SECONDS
    while True:
        try:
            config = requests.get(CONFIG_JSON_URL).json()
            print('Loaded twitter config')
        except requests.RequestException:
            config = fallback_config

        track = config.get('twitter', fallback_config.get('twitter', []))
        listener = TwitterStreamListener()
        stream = Stream(auth=api.auth, listener=listener)

        if first_run:
            first_run = False
            # Before following our own account, pull some existing self posts
            for tweet in reversed(api.user_timeline(TWITTER_ACCESS_TOKEN.split('-')[0])):
                if not hasattr(tweet, 'timestamp_ms'):
                    tweet.timestamp_ms = tweet.created_at.timestamp() * 1000
                listener.on_status(tweet)

        #stream.filter(track=track)
        #stream.filter(follow=['@beautrising'])
        #stream.filter(follow=['2717150874'])
        stream.userstream(encoding='utf-8')




def the_facebook_cycle():
    '''
    '''
    access_token = '|'.join((FACEBOOK_APP_ID, FACEBOOK_SECRET))
    graph = facebook.GraphAPI(access_token=access_token, version=FACEBOOK_API_VERSION)

    posted_ids = set()

    # Repeatedly re-configure about every RECONFIGURE_SECONDS
    while True:
        try:
            config = requests.get(CONFIG_JSON_URL).json()
            print('Loaded facebook config')
        except:
            config = fallback_config

        track = config.get('facebook', fallback_config.get('facebook', []))
        post_fields = 'id,message,from,name,link,created_time'
        profile_fields = 'picture,name,link'
        is_app_scoped = re.compile('app_scoped_user_id').search

        post_queue = []

        # For every tracked fb page or profile, we poll
        for id_or_alias in track:
            if id_or_alias.startswith('https://'):
                path = urlsplit(id_or_alias).path.split('/')[1:]
                if not path: # No hope for this url
                    continue
                id_or_alias = path[0]

            try:
                profile = graph.get_object(id_or_alias)
            except facebook.GraphAPIError: continue

            for post in graph.get_connections(profile['id'], 'posts', fields=post_fields).get('data', []):
                content = content_plain = content_html = post.get('message', '')
                link = post.get('link', '')
                name = post.get('name', link)
                if link:
                    content_plain = '{}\n{}'.format(content, link)
                    content_html = clean_html('{}\n<a href="{}">{}</a>'.format(content, link, name))

                # Fallback user info is empty
                user_name = id_or_alias
                user_link = ''
                image_https = ''

                poster_id = post.get('from', {}).get('id')
                if poster_id:
                    poster_profile = graph.get_object(poster_id, fields=profile_fields)

                    user_name = poster_profile.get('name', '')
                    user_link = poster_profile.get('link', '')
                    image_https = plucks(poster_profile, 'picture.data.url', '')

                dateobj = datetime.datetime.now()
                if post.get('created_time'):
                    dateobj = dateparser.parse(post['created_time'])
                timestamp_ms = int(dateobj.timestamp() * 1000)

                fb_post = NewsItem(
                    type='facebook',
                    content=content_plain,
                    content_html=content_html,
                    follow_link='',
                    image_http='',
                    image_https=image_https,
                    timestamp=timestamp_ms,
                    user_name=user_name,
                    user_handle=user_name,
                    user_link=user_link,
                )._asdict()

                if post['id'] not in posted_ids:
                    posted_ids.add(post['id'])
                    post_queue.append(fb_post)

        # Now sort the post_queue by timestamp and push them out
        for post in sorted(post_queue, key=lambda p: p['timestamp']):
            newsQ.put(post)

            # Signal to the NewsfeedServer process that new data is in the queue
            ws_command('new')

        # TODO: one cycle for polling and one cycle for reconfiguring
        time.sleep(RECONFIGURE_SECONDS)




if __name__ == '__main__':
    # Ensure that a restarted process has 
    NewsfeedServer.load_cache()
    atexit.register(NewsfeedServer.save_cache)

    # Start the blocking twitter listener process
    twitter = multiprocessing.Process(target=the_twitter_cycle)
    twitter.start()

    # Start the blocking facebook polling process
    fb = multiprocessing.Process(target=the_facebook_cycle)
    fb.start()

    # Start the blocking websocket server in this process
    if SSL_ENABLED:
        server = SimpleSSLWebSocketServer('', WEBSOCKET_PORT, NewsfeedServer,
            certfile=SSL_CERTFILE, keyfile=SSL_KEYFILE, version=ssl.PROTOCOL_TLSv1_2)
    else:
        server = SimpleWebSocketServer('', WEBSOCKET_PORT, NewsfeedServer)
    server.serveforever()

